import os
import time
import pandas as pd
from base_crawler import BaseCrawler  
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options


class LotteOnCrawler(BaseCrawler):
    """
    LotteON í˜ì´ì§€ì—ì„œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤.

    - ëŒ€ìƒ URL: ì½”ì¹´ì½œë¼ 190ml 60ìº” ì œí’ˆ í˜ì´ì§€
    - Seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ë¥¼ ì—´ê³  ë¦¬ë·°ë¥¼ ìˆ˜ì§‘
    - ìµœëŒ€ 500ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ë©´ ìë™ ì¢…ë£Œ
    - ìˆ˜ì§‘ëœ ì •ë³´ëŠ” ë‚ ì§œ, í‰ì , ë¦¬ë·°ê¸€ë¡œ êµ¬ì„±ë˜ì–´ CSV íŒŒì¼ë¡œ ì €ì¥

    Attributes:
        output_dir (str): ë¦¬ë·° ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        base_url (str): í¬ë¡¤ë§í•  ëŒ€ìƒ ìƒí’ˆ í˜ì´ì§€ URL
    """
    def __init__(self, output_dir: str):
        super().__init__(output_dir)
        self.base_url = "https://www.lotteon.com/p/product/LD755546264"  # ì½”ì¹´ì½œë¼ 190ml 60ìº”

    def scroll_until_review_loaded(self, scroll_count=5, delay=2):
        for _ in range(scroll_count):
            self.driver.execute_script("window.scrollBy(0, 1500);")
            time.sleep(delay)
            
    def scrape_reviews(self):
        options = Options()
        options.add_experimental_option("detach", True)
        options.add_experimental_option("excludeSwitches", ["enable-logging"])
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_window_size(1920, 1080)
        self.driver.get(self.base_url)
        time.sleep(3)

        values = []
        page = 1

        self.scroll_until_review_loaded(scroll_count=6)

        while True:
            try:
                review_elements = self.driver.find_elements(By.CSS_SELECTOR, '#reviewMain > div')
            except Exception as e:
                print(f"âŒ ë¦¬ë·° ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨: {e}")
                break

            for row in review_elements:
                try:
                    date = row.find_element(By.CSS_SELECTOR, 'span.date').text.strip()
                    star = float(row.find_element(By.CSS_SELECTOR, 'div.staring > em').text.strip())
                    review = row.find_element(By.CSS_SELECTOR, 'span.texting').text.strip().replace('\n', ' ').replace('\r', ' ')
                    values.append([date, star, review])

                    if len(values) >= 500:
                        print("500ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
                        self.reviews = values
                        self.driver.quit()
                        return
                except Exception:
                    continue

            try:
                next_btn = self.driver.find_element(By.CSS_SELECTOR, '#reviewMain .paginationArea .next')
                if 'disabled' in next_btn.get_attribute('class'):
                    print("ğŸš« ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ â€” ì¢…ë£Œ")
                    break
                self.driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(2)
                self.scroll_until_review_loaded(scroll_count=3)
                page += 1
            except Exception as e:
                print(f"âŒ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                break

        self.driver.quit()
        self.reviews = values


    def save_to_database(self):
        if not hasattr(self, 'reviews') or not self.reviews:
            print("âš ï¸ ì €ì¥í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.reviews, columns=['date', 'rate', 'review'])
        os.makedirs(self.output_dir, exist_ok=True)
        output_path = os.path.join(self.output_dir, 'reviews_lotteon.csv')
        df.to_csv(output_path, index=False, encoding='utf-8-sig', lineterminator='\n')
        print(f"ì €ì¥ ì™„ë£Œ: {output_path}")
