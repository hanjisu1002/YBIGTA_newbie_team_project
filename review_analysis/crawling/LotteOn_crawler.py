import os
import time
import pandas as pd
from base_crawler import BaseCrawler  
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class LotteOnCrawler(BaseCrawler):
    def __init__(self, output_dir: str):
        super().__init__(output_dir)
        self.base_url = "https://www.lotteon.com/p/product/LD755546264"  # ì½”ì¹´ì½œë¼ 190ml 60ìº”

    def start_browser(self):
        options = Options()
        options.add_experimental_option("detach", True)
        options.add_experimental_option("excludeSwitches", ["enable-logging"])
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_window_size(1920, 1080)
        self.driver.get(self.base_url)
        time.sleep(3)

    def scroll_until_review_loaded(self, scroll_count=5, delay=2):
        for i in range(scroll_count):
            print(f"â¬‡ï¸ ìŠ¤í¬ë¡¤ {i+1}/{scroll_count}")
            self.driver.execute_script("window.scrollBy(0, 1500);")
            time.sleep(delay)

    def wait_for_reviews(self, timeout=15):
        try:
            WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'div.review_list_wrap'))
            )
            print("â³ ë¦¬ë·° ì˜ì—­ ê°ì§€ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë¦¬ë·° ë¡œë”© ì‹¤íŒ¨: {e}")


    def scrape_reviews(self):
        values = []
        page = 1

        print("â–¶ ë¦¬ë·° ì˜ì—­ ì—¬ëŸ¬ ë²ˆ ìŠ¤í¬ë¡¤ ì¤‘...")
        self.scroll_until_review_loaded(scroll_count=6)

        while True:
            print(f"\nğŸ“„ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

            try:
                review_elements = self.driver.find_elements(By.CSS_SELECTOR, '#reviewMain > div')
            except Exception as e:
                print(f"âŒ ë¦¬ë·° ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨: {e}")
                break

            for idx, row in enumerate(review_elements):
                try:
                    date = row.find_element(By.CSS_SELECTOR, 'span.date').text.strip()
                    star = float(row.find_element(By.CSS_SELECTOR, 'div.staring > em').text.strip())
                    review = row.find_element(By.CSS_SELECTOR, 'span.texting').text.strip()
                    values.append([date, star, review])
                except Exception:
                    continue

            print(f"ğŸ“¦ {page}í˜ì´ì§€ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ, ì´ ìˆ˜ì§‘ ìˆ˜: {len(values)}")

        # ë‹¤ìŒ ë²„íŠ¼ ì²˜ë¦¬
            try:
                next_btn = self.driver.find_element(By.CSS_SELECTOR, '#reviewMain .paginationArea .next')
                if 'disabled' in next_btn.get_attribute('class'):
                    print("ğŸš« ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ â€” ì¢…ë£Œ")
                    break
                self.driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(2)
                self.scroll_until_review_loaded(scroll_count=3)
                page += 1
            except Exception as e:
                print(f"âŒ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                break

        print(f"\nâœ… ì´ {len(values)}ê°œì˜ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
        self.reviews = values

    def save_to_database(self):
        if not hasattr(self, 'reviews') or not self.reviews:
            print("âš ï¸ ì €ì¥í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.reviews, columns=['date', 'star_rate', 'review'])
        os.makedirs(self.output_dir, exist_ok=True)
        output_path = os.path.join(self.output_dir, 'reviews_lotteon.csv')
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
